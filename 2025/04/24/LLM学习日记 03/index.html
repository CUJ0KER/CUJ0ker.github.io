
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <title>LLM学习日记 03 | 『1nv_qaq&#39;s Blog』</title>
    <meta name="author" content="1nv-qaq" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <meta
        name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"
    />
    <link rel="icon" href="/images/avatar.jpg" />
    <link rel="preconnect" href="https://s4.zstatic.net" />
<script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.googleapis.cn" />
<link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin />
<link
    rel="stylesheet"
    href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap"
/>
<script> const mixins = {}; </script>

<script src="https://polyfill.alicdn.com/v3/polyfill.min.js?features=default"></script>


<script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>


<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.css" />
<script src="/js/lib/math.js"></script>


<script src="/js/lib/preview.js"></script>









<link rel="stylesheet" href="/css/main.css" />

<meta name="generator" content="Hexo 7.3.0"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>wait wait wait...</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>『1NV_QAQ&#39;S BLOG』</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;『1NV_QAQ&#39;S BLOG』</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div class="article">
    <div>
        <h1>LLM学习日记 03</h1>
    </div>
    <div class="info">
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/4/24
        </span>
        
        <span class="category">
            <a href="/categories/AI-Algorithm/">
                <span class="icon">
                    <i class="fa-solid fa-bookmark fa-fw"></i>
                </span>
                AI Algorithm
            </a>
        </span>
        
        
        <span class="tags">
            <span class="icon">
                <i class="fa-solid fa-tags fa-fw"></i>
            </span>
            
            
            <span class="tag">
                
                <a href="/tags/AI-Algorithm/" style="color: #ff7d73">
                    AI Algorithm
                </a>
            </span>
            
        </span>
        
    </div>
    
    <div class="content" v-pre>
        <ul>
<li>Positional Encoding</li>
<li>LLM Structure</li>
<li>解码采样策略<span id="more"></span></li>
</ul>
<h1 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h1><p>Transformer丢掉了时序信息，而一个句子中词与词之间的顺序不同会导致千差万别的语义，为了解决时序问题，Transformer的作者采用了Positional Encoding的办法，即位置编码<br>简单概括，Positional Encoding就是将位置信息嵌入到Embedding词向量中，让Transformer保留词向量的位置信息<br>进一步，我们期望得到一种位置表示方式，满足：</p>
<ul>
<li>能用来表示一个token在序列中的绝对位置</li>
<li>在序列长度不同的情况下，不同序列中token的相对位置&#x2F;距离也保持一致</li>
<li>可以用来表示模型在训练过程中从来没有看到过的句子长度，称为长度外推问题</li>
</ul>
<h2 id="绝对位置编码"><a href="#绝对位置编码" class="headerlink" title="绝对位置编码"></a>绝对位置编码</h2><h3 id="Transformer的位置编码"><a href="#Transformer的位置编码" class="headerlink" title="Transformer的位置编码"></a>Transformer的位置编码</h3><ul>
<li>$PE_{t}&#x3D;[sin(\omega_{0}t),cos(\omega_{0}t),sin(\omega_{1}t),cos(\omega_{1}t),\cdots,sin(\omega_{\frac{d_{model}}{2}-1}t),cos(\omega_{\frac{d_{model}}{2}-1}t)]$</li>
<li>采用正余弦函数交替进行位置编码</li>
<li>可以通过线性变换矩阵得到其他位置的表示，因为<br>$$\begin{pmatrix}sin(t+\Delta t)\cos(t+\Delta t)\end{pmatrix}&#x3D;\begin{pmatrix}cos\Delta t&amp;sin\Delta t\-sin\Delta t&amp;cos\Delta t\end{pmatrix}\begin{pmatrix}sint\cost\end{pmatrix}$$<br><img src="https://pub-624516a8d051462080df06b1964d2889.r2.dev/LLM%2003/01.png" alt="img"></li>
</ul>
<pre><code class="python">class PositionalEncoding(nn.Module):

    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()       
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        #pe.requires_grad = False
        self.register_buffer(&#39;pe&#39;, pe)

    def forward(self, x):
        return x + self.pe[:x.size(0), :]
</code></pre>
<ul>
<li>Transformer位置编码的缺点：位置编码点积的无向性，$PE_{t}^{T}*PE_{t+\Delta t}&#x3D;PE_{t}^{T}*PE_{t-\Delta t}$，即两个位置编码的乘积仅取决于$\Delta T$，不能用来表示位置的方向性</li>
</ul>
<h3 id="BERT的可学习位置编码"><a href="#BERT的可学习位置编码" class="headerlink" title="BERT的可学习位置编码"></a>BERT的可学习位置编码</h3><ul>
<li>将位置编码当作可训练参数</li>
<li>缺点是没有长度外推性</li>
</ul>
<h1 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h1><p>LLM的结构可以分为四类：</p>
<ul>
<li>Decoder-only<ul>
<li>从左到右的单向注意力</li>
<li>自回归语言模型</li>
<li>文本生产效果好</li>
<li>训练效率高</li>
<li>GPT、llama、BLOOM、OPT<br><img src="https://pub-624516a8d051462080df06b1964d2889.r2.dev/LLM%2003/02.png" alt="img"></li>
</ul>
</li>
<li>Encoder-only<ul>
<li>经典模型BERT</li>
<li>目标是生成语言模型，预训练的语言模型</li>
</ul>
</li>
<li>Encoder-Decoder<ul>
<li>输入双向注意力、输出单向注意力</li>
<li>对问题的编码理解更充分，在偏理解的任务上表现较好</li>
<li>训练效率低</li>
<li>文本生成任务表现差</li>
<li>T5、Flan-T5、BART</li>
</ul>
</li>
<li>Prefix LM（特殊的Encoder-Decoder）<ul>
<li>GLM、U-PaLm</li>
</ul>
</li>
<li>MOE (Mixture of Experts)<ul>
<li>Deepseek V2&#x2F;V3、Mistral<br><img src="https://pub-624516a8d051462080df06b1964d2889.r2.dev/LLM%2003/03.png" alt="img"></li>
</ul>
</li>
</ul>
<h1 id="解码采样策略"><a href="#解码采样策略" class="headerlink" title="解码采样策略"></a>解码采样策略</h1><h2 id="Greedy-Search"><a href="#Greedy-Search" class="headerlink" title="Greedy Search"></a>Greedy Search</h2><ul>
<li>每次选择概率最大的token解码</li>
<li>简单高效</li>
<li>导致生成的文本过于单调和重复</li>
</ul>
<pre><code class="python">import torch
import torch.nn.functional as F

def greedy_search(logits, max_len=20, eos_token_id=2):
    batch_size = logits.size(0)
    seq_len = logits.size(1)
    vocab_size = logits.size(2)
    
    # 初始化生成的序列，存储每个时间步选择的词索引
    generated_sequence = torch.zeros(batch_size, max_len, dtype=torch.long)
    # 记录当前生成的序列，初始化为开始 token（一般为0或者某个特殊的起始符号）
    current_input = torch.zeros(batch_size, dtype=torch.long)  # 起始token id  
    for t in range(max_len):
        # 获取当前步骤的 logits
        current_logits = logits[:, t, :]        
        # 计算每个词的概率分布（通过softmax）
        probs = F.softmax(current_logits, dim=-1)       
        # 选择概率最高的词索引（greedy）
        next_token = torch.argmax(probs, dim=-1)        
        # 将选择的词加入到生成的序列中
        generated_sequence[:, t] = next_token        
        # 检查是否遇到结束标记，若遇到则停止生成
        if (next_token == eos_token_id).all():
            break        
        # 更新当前输入（下一个时间步的输入）
        current_input = next_token
    return generated_sequence
</code></pre>
<h2 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h2><ul>
<li>维护一个大小为$k$的候选序列集合，每一步从每个候选序列的概率分布中选择概率最高的$k$个单词</li>
<li>保留总概率最高的$k$个候选序列</li>
<li>平衡生成的质量和多样性</li>
<li>可能导致生成的文本过于保守和不自然</li>
</ul>
<pre><code class="python">def pred(input):
    batch,seq_len=input.shape
    generate=torch.randn(size=(batch,1,10))
    return generate

def beam_search(input_ids,max_length,num_beams):
    batch=input_ids.shape[0]

    #输入扩展：效果是把input_ids复制成 batch_size * beam_size的个数
    expand_size=num_beams
    expanded_return_idx = (
        torch.arange(input_ids.shape[0]).view(-1, 1).repeat(1, expand_size).view(-1).to(input_ids.device)
    )
    input_ids = input_ids.index_select(0, expanded_return_idx)
    print(input_ids)

    batch_beam_size,cur_len=input_ids.shape
    beam_scores=torch.zeros(size=(batch,num_beams),dtype=torch.float,device=input_ids.device)
    beam_scores[:,1:]=-1e9

    beam_scores=beam_scores.view(size=(batch*num_beams,))
    next_tokens=torch.zeros(size=(batch,num_beams),dtype=torch.long,device=input_ids.device)
    next_indices=torch.zeros(size=(batch,num_beams),dtype=torch.long,device=input_ids.device)

    while cur_len&lt;max_length:

        logits=pred(input_ids)    #batch,seq_len,vocab
        next_token_logits=logits[:,-1,:]  #当前时刻的输出

        #归一化
        next_token_scores=F.log_softmax(next_token_logits,dim=-1)   # (batch_size * num_beams, vocab_size)
        #求概率
        next_token_scores = next_token_scores + beam_scores[:, None].expand_as(next_token_scores)  # 当前概率+先前概率

        # reshape for beam search
        vocab_size = next_token_scores.shape[-1]
        next_token_scores = next_token_scores.view(batch, num_beams * vocab_size)

        # 当前时刻的token 得分，  token_id
        next_token_scores, next_tokens = torch.topk(
            next_token_scores, num_beams, dim=1, largest=True, sorted=True
        )

        next_indices = next_tokens // vocab_size  #对应的beam_id
        next_tokens = next_tokens % vocab_size    #对应的indices

        #集束搜索核心
        def process(input_ids,next_scores,next_tokens,next_indices):
            batch_size=3
            group_size=3
            next_beam_scores = torch.zeros((batch_size, num_beams), dtype=next_scores.dtype)
            next_beam_tokens = torch.zeros((batch_size, num_beams), dtype=next_tokens.dtype)
            next_beam_indices = torch.zeros((batch_size,num_beams), dtype=next_indices.dtype)

            for batch_idx in range(batch_size):
                beam_idx=0
                for beam_token_rank, (next_token, next_score, next_index) in enumerate(
                        zip(next_tokens[batch_idx], next_scores[batch_idx], next_indices[batch_idx])
                ):
                    batch_beam_idx=batch_idx*num_beams+next_index

                    next_beam_scores[batch_idx, beam_idx] = next_score      #当前路径得分
                    next_beam_tokens[batch_idx, beam_idx] = next_token      #当前时刻的token
                    next_beam_indices[batch_idx, beam_idx] = batch_beam_idx  #先前对应的id

                    beam_idx += 1

            return next_beam_scores.view(-1), next_beam_tokens.view(-1), next_beam_indices.view(-1)

        beam_scores, beam_next_tokens, beam_idx=process(input_ids,next_token_scores,next_tokens,next_indices)

        # 更新输入， 找到对应的beam_idx, 选择的tokens, 拼接为新的输入      #(batch*beam,seq_len)
        input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)
        cur_len = cur_len + 1
    #输出
    return input_ids,beam_scores

if __name__ == &#39;__main__&#39;:
    input_ids=torch.randint(0,100,size=(3,1))
    print(input_ids)
    input_ids,beam_scores=beam_search(input_ids,max_length=10,num_beams=3)
    print(input_ids)
</code></pre>
<h2 id="Top-K-Sampling"><a href="#Top-K-Sampling" class="headerlink" title="Top-K Sampling"></a>Top-K Sampling</h2><ul>
<li>从排名前$k$的token中进行抽样，允许其他分数或概率较高的token被选中</li>
<li>将采样池限制为固定大小的k可能在分布比较尖锐的时候胡言乱语，另一方面在分布比较平坦的时候限制模型的创造力</li>
</ul>
<pre><code class="python">#@torch.inference_mode()
    @torch.no_grad()
    def generate(self, idx, 
                 eos, max_new_tokens, temperature=1.0, top_k=None):
        for _ in range(max_new_tokens):
            # if the sequence context is growing too long we must crop it at block_size
            idx_cond = idx if idx.size(1) &lt;= self.params.max_seq_len else idx[:, -self.params.max_seq_len:]
            # forward the model to get the logits for the index in the sequence
            logits = self(idx_cond)##===================================
            logits = logits[:, -1, :] # crop to just the final time step
            if temperature == 0.0:
                # &quot;sample&quot; the single most likely index
                _, idx_next = torch.topk(logits, k=1, dim=-1)
            else:
                # pluck the logits at the final step and scale by desired temperature
                logits = logits / temperature
                # optionally crop the logits to only the top k options
                if top_k is not None:
                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                    logits[logits &lt; v[:, [-1]]] = -float(&#39;Inf&#39;)
                # apply softmax to convert logits to (normalized) probabilities
                probs = F.softmax(logits, dim=-1)
                idx_next = torch.multinomial(probs, num_samples=1)
            # append sampled index to the running sequence and continue
            idx = torch.cat((idx, idx_next), dim=1)
            if idx_next==eos:
                break
        return idx
</code></pre>
<h2 id="Top-P-Sampling"><a href="#Top-P-Sampling" class="headerlink" title="Top-P Sampling"></a>Top-P Sampling</h2><ul>
<li>在累积概率超过设定值$P$的最小单词集中采样</li>
</ul>
<pre><code class="python">import torch
import torch.nn.functional as F

def top_p_sampling(logits, p=0.9):
    # 应用softmax得到概率分布
    probs = F.softmax(logits, dim=-1)
    # 对概率分布进行排序
    sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)    
    # 计算累积概率
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)    
    # 找到第一个使得累积概率大于p的位置
    cutoff_idx = torch.sum(cumulative_probs &lt;= p, dim=-1)
    # 根据cutoff_idx截断概率分布
    masked_probs = torch.gather(probs, dim=-1, index=sorted_indices[:, :cutoff_idx + 1])    
    # 重新归一化剩余概率
    masked_probs = masked_probs / masked_probs.sum(dim=-1, keepdim=True)
    # 从候选词中随机选择一个词
    selected_idx = torch.multinomial(masked_probs, 1)    
    return sorted_indices.gather(-1, selected_idx)
</code></pre>
<h2 id="Majority-Vote"><a href="#Majority-Vote" class="headerlink" title="Majority Vote"></a>Majority Vote</h2><ul>
<li>多数投票，输出多个回答，选择答案一致性最多的作为最终答案</li>
</ul>

    </div>
    
    
    
    
    
    
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2024 - 2025 『1nv_qaq&#39;s Blog』
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;1nv-qaq
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    <script src="/js/<fireworks.min.js>"></script>


    <canvas
    id="fireworks"
    style="position: fixed; top: 0; left: 0; width: 100vw; height: 100vh; pointer-events: none; z-index: 32767"
></canvas>
<script src="https://s4.zstatic.net/ajax/libs/animejs/3.2.1/anime.min.js"></script>
<script src="/js/fireworks.min.js"></script>




    
    




    
</body>
</html>
